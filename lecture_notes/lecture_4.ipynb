{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "klmu3ZG08PPr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('../names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BCQomLE_8PPs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V_zt2QHr8PPs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZlFLjQyT8PPu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "8ofj1s6d8PPv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.335385322570801"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## dlogprobs\n",
        "# loss \t= - (a + b + c) / 3\n",
        "#\t\t= -1/3a + -1/3b + - 1/3c\n",
        "#dloss/da\t= -1/3\n",
        "#general: = -1/n for each of the values\n",
        "#all other values (that are not plucked out), have gradient 0, becasus they don't impact loss\n",
        "# so: start with tensor of same shape as logprobs, set everything to zero, set gradient of plucled out values to -1/n\n",
        "torch.zeros_like(logprobs).shape\n",
        "\n",
        "##dprobs\n",
        "# dLoss / dprobs = dLoss / dlogprobs * dlogprobs / dprobs\n",
        "# \t= dlogprobs * 1.0/probs (derivate of log(x) is 1/x)\n",
        "\n",
        "##dcounts_sum_inv\n",
        "# it's a broadcasted tensor, so that needs to be taken into account\n",
        "# need to sum the broadcasted tensor, because it's used multiple times\n",
        "\n",
        "##dcounts\n",
        "# b = a1 + a2 + a3 -> gets its gradient from b\n",
        "\n",
        "##dlogits\n",
        "# dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "# create a vector that has a 1 at each index of the max, rest 0's. then multiply that with global gradient\n",
        "\n",
        "##dh\n",
        "# need to transpose W2 and then matrix multiply with dlogits\n",
        "# a derivative of a matrix multiplication is a matrix multiplication\n",
        "# can use shapes as a guide to which ones to multiply, dh.shape needs to be h.shape\n",
        "\n",
        "##db2\n",
        "# needs to be of shape [27], so need to sum up dlogits over dimension 0, to get rid of 0th dimension and keep 1st (27)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "mO-8aqxK8PPw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# -----------------\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = dlogprobs * 1.0/probs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
        "dbndiff2 =1.0/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += 2*bndiff * dbndiff2\n",
        "dbnmeani = (-1.0 * dbndiff).sum(0, keepdim=True)\n",
        "dhprebn = dbndiff.clone()\n",
        "dhprebn += (1.0/n) * torch.ones_like(hprebn) * dbnmeani\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "\tfor j in range(Xb.shape[1]):\n",
        "\t\tix = Xb[k,j]\n",
        "\t\tdC[ix] += demb[k, j]\n",
        "\n",
        "\n",
        "# -----------------\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebLtYji_8PPw"
      },
      "outputs": [],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.7463,  1.0941, -0.6237,  0.4439, -0.4217,  1.0033, -0.2464,  0.0437,\n",
              "        -0.8170, -0.0185,  0.2387,  0.1556,  0.1101, -0.2142,  0.1462, -0.8529,\n",
              "        -1.3496, -0.4872, -0.7139,  0.4762,  0.4624, -0.4490, -0.3492,  0.8074,\n",
              "         0.6009, -0.2516, -0.3523], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1281bc290>"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x12da550d0>"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(logits.detach(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "-gCXbB4C8PPx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "# -----------------\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd-MkhB68PPy"
      },
      "outputs": [],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "POdeZSKT8PPy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "wPy8DhqB8PPz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7467\n",
            "  10000/ 200000: 2.1389\n",
            "  20000/ 200000: 2.3914\n",
            "  30000/ 200000: 2.4475\n",
            "  40000/ 200000: 1.9978\n",
            "  50000/ 200000: 2.4888\n",
            "  60000/ 200000: 2.3514\n",
            "  70000/ 200000: 2.0095\n",
            "  80000/ 200000: 2.3558\n",
            "  90000/ 200000: 2.1387\n",
            " 100000/ 200000: 1.9460\n",
            " 110000/ 200000: 2.2867\n",
            " 120000/ 200000: 2.0349\n",
            " 130000/ 200000: 2.3813\n",
            " 140000/ 200000: 2.3839\n",
            " 150000/ 200000: 2.2015\n",
            " 160000/ 200000: 1.9493\n",
            " 170000/ 200000: 1.8810\n",
            " 180000/ 200000: 1.9452\n",
            " 190000/ 200000: 1.8709\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    #2nd layer\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    #tanh\n",
        "    dhpreact = (1 - h**2) * dh\n",
        "    #batch norm\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    #1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    #embeddings\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k, j]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "ZEpI0hMW8PPz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
            "(30, 200)       | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n",
            "(200,)          | exact: False | approximate: True  | maxdiff: 3.4924596548080444e-09\n",
            "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(27,)           | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
          ]
        }
      ],
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "6aFnP_Zc8PP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 2.0727033615112305\n",
            "val 2.112077236175537\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "xHeQNv3s8PP1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carmah.\n",
            "ambrileigh.\n",
            "mri.\n",
            "reet.\n",
            "khalaysie.\n",
            "mahnen.\n",
            "delyah.\n",
            "jareei.\n",
            "nellara.\n",
            "chaiivin.\n",
            "leigh.\n",
            "ham.\n",
            "joce.\n",
            "quint.\n",
            "shon.\n",
            "marianni.\n",
            "waythoniearyn.\n",
            "kai.\n",
            "eveigh.\n",
            "brex.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
